{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Dataset successfully split into train, test, and valid folders.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "# Define paths\n",
    "source_dir = r\"C:\\Users\\bhish\\Downloads\\FullAndFinalPlantVillage\\Plant Village Dataset\\Train\"\n",
    "destination_dir =  \"PlantVillage_100_DATASETBHISHAMGAHLAUT\"\n",
    "\n",
    "# Folders for train, test, and validation\n",
    "splits = [\"train\", \"test\", \"valid\"]\n",
    "split_sizes = {\"train\": 200, \"test\": 50, \"valid\": 50}\n",
    "\n",
    "# Create destination folders if not exist\n",
    "for split in splits:\n",
    "    os.makedirs(os.path.join(destination_dir, split), exist_ok=True)\n",
    "\n",
    "# Process each category\n",
    "for category in os.listdir(source_dir):\n",
    "    category_path = os.path.join(source_dir, category)\n",
    "\n",
    "    # Skip if not a directory\n",
    "    if not os.path.isdir(category_path):\n",
    "        continue\n",
    "\n",
    "    # Get all images in category folder\n",
    "    images = sorted(os.listdir(category_path))  # Sorting ensures order\n",
    "\n",
    "    # Create category subfolders in train, test, and valid\n",
    "    for split in splits:\n",
    "        os.makedirs(os.path.join(destination_dir, split, category), exist_ok=True)\n",
    "\n",
    "    # Copy images\n",
    "    start_idx = 0\n",
    "    for split in splits:\n",
    "        num_images = split_sizes[split]\n",
    "        split_dest = os.path.join(destination_dir, split, category)\n",
    "\n",
    "        for img in images[start_idx:start_idx + num_images]:\n",
    "            src = os.path.join(category_path, img)\n",
    "            dest = os.path.join(split_dest, img)\n",
    "            shutil.copy(src, dest)\n",
    "\n",
    "        start_idx += num_images\n",
    "\n",
    "print(\"âœ… Dataset successfully split into train, test, and valid folders.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4172 images belonging to 18 classes.\n",
      "Found 1141 images belonging to 18 classes.\n",
      "Found 1140 images belonging to 18 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bhish\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
      "C:\\Users\\bhish\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "\u001b[1m131/131\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m237s\u001b[0m 2s/step - accuracy: 0.1551 - loss: 2.8586 - val_accuracy: 0.4049 - val_loss: 1.9083\n",
      "Epoch 2/15\n",
      "\u001b[1m131/131\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m156s\u001b[0m 1s/step - accuracy: 0.3611 - loss: 1.9961 - val_accuracy: 0.5451 - val_loss: 1.4579\n",
      "Epoch 3/15\n",
      "\u001b[1m131/131\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m154s\u001b[0m 1s/step - accuracy: 0.4136 - loss: 1.7655 - val_accuracy: 0.6240 - val_loss: 1.2131\n",
      "Epoch 4/15\n",
      "\u001b[1m131/131\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m152s\u001b[0m 1s/step - accuracy: 0.4692 - loss: 1.5941 - val_accuracy: 0.6231 - val_loss: 1.1523\n",
      "Epoch 5/15\n",
      "\u001b[1m131/131\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m161s\u001b[0m 1s/step - accuracy: 0.4784 - loss: 1.5148 - val_accuracy: 0.6152 - val_loss: 1.2286\n",
      "Epoch 6/15\n",
      "\u001b[1m131/131\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m152s\u001b[0m 1s/step - accuracy: 0.5240 - loss: 1.3669 - val_accuracy: 0.5890 - val_loss: 1.2071\n",
      "Epoch 7/15\n",
      "\u001b[1m131/131\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m150s\u001b[0m 1s/step - accuracy: 0.5186 - loss: 1.3485 - val_accuracy: 0.6661 - val_loss: 0.9241\n",
      "Epoch 8/15\n",
      "\u001b[1m131/131\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m150s\u001b[0m 1s/step - accuracy: 0.5488 - loss: 1.2673 - val_accuracy: 0.6310 - val_loss: 1.0186\n",
      "Epoch 9/15\n",
      "\u001b[1m131/131\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m151s\u001b[0m 1s/step - accuracy: 0.5638 - loss: 1.2465 - val_accuracy: 0.6757 - val_loss: 0.9237\n",
      "Epoch 10/15\n",
      "\u001b[1m131/131\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m151s\u001b[0m 1s/step - accuracy: 0.5684 - loss: 1.2173 - val_accuracy: 0.6976 - val_loss: 0.8510\n",
      "Epoch 11/15\n",
      "\u001b[1m131/131\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m150s\u001b[0m 1s/step - accuracy: 0.5950 - loss: 1.1285 - val_accuracy: 0.6933 - val_loss: 0.8801\n",
      "Epoch 12/15\n",
      "\u001b[1m131/131\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m151s\u001b[0m 1s/step - accuracy: 0.6068 - loss: 1.1155 - val_accuracy: 0.6862 - val_loss: 1.0581\n",
      "Epoch 13/15\n",
      "\u001b[1m131/131\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m151s\u001b[0m 1s/step - accuracy: 0.6510 - loss: 1.0593 - val_accuracy: 0.7809 - val_loss: 0.6787\n",
      "Epoch 14/15\n",
      "\u001b[1m131/131\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m151s\u001b[0m 1s/step - accuracy: 0.6526 - loss: 0.9874 - val_accuracy: 0.7721 - val_loss: 0.6798\n",
      "Epoch 15/15\n",
      "\u001b[1m131/131\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m150s\u001b[0m 1s/step - accuracy: 0.6356 - loss: 0.9973 - val_accuracy: 0.7835 - val_loss: 0.6182\n",
      "âœ… Model saved at C:\\Users\\bhish\\OneDrive\\Desktop\\Agriculture_Project\\Backend\\WeatherAnalysis\\my_model_ALL.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 121ms/step\n",
      "ğŸ“¸ Image: 19f6234f-30e6-41e1-90bf-8204612bb150___R.S_HL 8223 copy.jpg\n",
      "âœ… Actual Class: Corn (Maize) - Healthy\n",
      "ğŸ” Predicted Class: Corn (Maize) - Healthy (Confidence: 100.0%)\n",
      "--------------------------------------------------\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step\n",
      "ğŸ“¸ Image: 9ffb3724-b220-4e79-9883-f1d3a5141152___RS_GLSp 4688 copy.jpg\n",
      "âœ… Actual Class: Corn (Maize) - Cercospora Leaf Spot\n",
      "ğŸ” Predicted Class: Corn (Maize) - Northern Leaf Blight (Confidence: 74.38999938964844%)\n",
      "--------------------------------------------------\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step\n",
      "ğŸ“¸ Image: 0b799242-624f-46fb-acd3-a9352e8b4213___R.S_HL 8216 copy 2_flipLR.jpg\n",
      "âœ… Actual Class: Corn (Maize) - Healthy\n",
      "ğŸ” Predicted Class: Corn (Maize) - Healthy (Confidence: 100.0%)\n",
      "--------------------------------------------------\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step\n",
      "ğŸ“¸ Image: 086c8cca-951a-4c1d-93b9-e225b628f2fd___R.S_HL 8024 copy.jpg\n",
      "âœ… Actual Class: Corn (Maize) - Healthy\n",
      "ğŸ” Predicted Class: Corn (Maize) - Healthy (Confidence: 100.0%)\n",
      "--------------------------------------------------\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step\n",
      "ğŸ“¸ Image: 9fc7cf37-b583-491c-adc3-a391edc73aae___R.S_HL 7973 copy 2_flipLR.jpg\n",
      "âœ… Actual Class: Corn (Maize) - Healthy\n",
      "ğŸ” Predicted Class: Corn (Maize) - Healthy (Confidence: 99.98999786376953%)\n",
      "--------------------------------------------------\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step\n",
      "ğŸ“¸ Image: ff339e47-52bb-4e16-8338-d056f7dcf525___R.S_HL 8108 copy_flipLR.jpg\n",
      "âœ… Actual Class: Corn (Maize) - Healthy\n",
      "ğŸ” Predicted Class: Corn (Maize) - Healthy (Confidence: 100.0%)\n",
      "--------------------------------------------------\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step\n",
      "ğŸ“¸ Image: 00031d74-076e-4aef-b040-e068cd3576eb___R.S_HL 8315 copy 2_flipLR.jpg\n",
      "âœ… Actual Class: Corn (Maize) - Healthy\n",
      "ğŸ” Predicted Class: Corn (Maize) - Healthy (Confidence: 100.0%)\n",
      "--------------------------------------------------\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step\n",
      "ğŸ“¸ Image: d792cd67-6f5b-45aa-9e77-b6286057eac0___R.S_HL 8123 copy.jpg\n",
      "âœ… Actual Class: Corn (Maize) - Healthy\n",
      "ğŸ” Predicted Class: Corn (Maize) - Healthy (Confidence: 100.0%)\n",
      "--------------------------------------------------\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step\n",
      "ğŸ“¸ Image: 193e6262-e589-4f7d-93b8-c707368ee3b0___R.S_HL 8262 copy_flipLR.jpg\n",
      "âœ… Actual Class: Corn (Maize) - Healthy\n",
      "ğŸ” Predicted Class: Corn (Maize) - Healthy (Confidence: 100.0%)\n",
      "--------------------------------------------------\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step\n",
      "ğŸ“¸ Image: 270ba8ed-21f5-43cc-b401-7b643ce5f65d___R.S_HL 8090 copy 2_flipLR.jpg\n",
      "âœ… Actual Class: Corn (Maize) - Healthy\n",
      "ğŸ” Predicted Class: Corn (Maize) - Healthy (Confidence: 100.0%)\n",
      "--------------------------------------------------\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step\n",
      "ğŸ“¸ Image: 1dc85ac0-f1fb-4acc-b50b-5625d4a53f87___R.S_HL 7900 copy 2.jpg\n",
      "âœ… Actual Class: Corn (Maize) - Healthy\n",
      "ğŸ” Predicted Class: Corn (Maize) - Healthy (Confidence: 100.0%)\n",
      "--------------------------------------------------\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step\n",
      "ğŸ“¸ Image: dcac2db4-95f4-4209-97fc-bccef6b0acca___R.S_HL 5537 copy 2_flipLR.jpg\n",
      "âœ… Actual Class: Corn (Maize) - Healthy\n",
      "ğŸ” Predicted Class: Corn (Maize) - Healthy (Confidence: 100.0%)\n",
      "--------------------------------------------------\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step\n",
      "ğŸ“¸ Image: 18a8a550-10c1-47e0-8049-fba853f92918___R.S_HL 5497 copy_flipLR.jpg\n",
      "âœ… Actual Class: Corn (Maize) - Healthy\n",
      "ğŸ” Predicted Class: Corn (Maize) - Healthy (Confidence: 100.0%)\n",
      "--------------------------------------------------\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step\n",
      "ğŸ“¸ Image: 8977f9a8-3384-4a8c-ac81-ebb180d7a442___R.S_HL 8035 copy.jpg\n",
      "âœ… Actual Class: Corn (Maize) - Healthy\n",
      "ğŸ” Predicted Class: Corn (Maize) - Healthy (Confidence: 100.0%)\n",
      "--------------------------------------------------\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n",
      "ğŸ“¸ Image: 07078bdc-44b5-4103-8f13-bf889f7371c5___R.S_HL 8068 copy_flipLR.jpg\n",
      "âœ… Actual Class: Corn (Maize) - Healthy\n",
      "ğŸ” Predicted Class: Corn (Maize) - Healthy (Confidence: 100.0%)\n",
      "--------------------------------------------------\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step\n",
      "ğŸ“¸ Image: ec1634bd-5b87-4072-9306-176a85f58ba6___R.S_HL 8202 copy 3.jpg\n",
      "âœ… Actual Class: Corn (Maize) - Healthy\n",
      "ğŸ” Predicted Class: Corn (Maize) - Healthy (Confidence: 100.0%)\n",
      "--------------------------------------------------\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n",
      "ğŸ“¸ Image: 0b5492ee-253d-4a13-ad5a-780cb954b09a___RS_GLSp 4615 copy_new30degFlipLR.jpg\n",
      "âœ… Actual Class: Corn (Maize) - Cercospora Leaf Spot\n",
      "ğŸ” Predicted Class: Corn (Maize) - Cercospora Leaf Spot (Confidence: 85.76000213623047%)\n",
      "--------------------------------------------------\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n",
      "ğŸ“¸ Image: 2c52d7ed-d259-4ce3-b38a-9ecdb86b7e85___R.S_HL 8065 copy_flipLR.jpg\n",
      "âœ… Actual Class: Corn (Maize) - Healthy\n",
      "ğŸ” Predicted Class: Corn (Maize) - Healthy (Confidence: 100.0%)\n",
      "--------------------------------------------------\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step\n",
      "ğŸ“¸ Image: de4b44f2-76c6-40bf-b48a-d69d458bb68d___RS_GLSp 4638 copy 2.jpg\n",
      "âœ… Actual Class: Corn (Maize) - Cercospora Leaf Spot\n",
      "ğŸ” Predicted Class: Corn (Maize) - Cercospora Leaf Spot (Confidence: 69.04000091552734%)\n",
      "--------------------------------------------------\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step\n",
      "ğŸ“¸ Image: 5d69a1f2-cead-4661-8014-63e35e131850___RS_GLSp 4337 copy_90deg.jpg\n",
      "âœ… Actual Class: Corn (Maize) - Cercospora Leaf Spot\n",
      "ğŸ” Predicted Class: Corn (Maize) - Cercospora Leaf Spot (Confidence: 95.77999877929688%)\n",
      "--------------------------------------------------\n",
      "ğŸ¯ Model Accuracy on 20 Test Images: 95.00% (19/20 Correct Predictions)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import random\n",
    "\n",
    "# âœ… Dataset Paths (Modify These Paths as Needed)\n",
    "train_dir = r\"C:\\Users\\bhish\\OneDrive\\Desktop\\ML\\NEWMODELS\\PlantVillage_100_DATASETBHISHAMGAHLAUT\\train\"\n",
    "val_dir = r\"C:\\Users\\bhish\\OneDrive\\Desktop\\ML\\NEWMODELS\\PlantVillage_100_DATASETBHISHAMGAHLAUT\\valid\"\n",
    "test_dir = r\"C:\\Users\\bhish\\OneDrive\\Desktop\\ML\\NEWMODELS\\PlantVillage_100_DATASETBHISHAMGAHLAUT\\test\"\n",
    "\n",
    "# âœ… Image Parameters\n",
    "IMAGE_SIZE = (256, 256)\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# âœ… Data Augmentation & Normalization\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,  # Normalize pixel values\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode=\"nearest\"\n",
    ")\n",
    "\n",
    "val_datagen = ImageDataGenerator(rescale=1./255)  # Only Normalization\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)  # Normalize test images\n",
    "\n",
    "# âœ… Load Dataset\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_dir, target_size=IMAGE_SIZE, batch_size=BATCH_SIZE, class_mode='categorical')\n",
    "\n",
    "val_generator = val_datagen.flow_from_directory(\n",
    "    val_dir, target_size=IMAGE_SIZE, batch_size=BATCH_SIZE, class_mode='categorical')\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    test_dir, target_size=IMAGE_SIZE, batch_size=1, class_mode='categorical', shuffle=False)\n",
    "\n",
    "# âœ… Get Class Labels\n",
    "CLASS_NAMES = list(train_generator.class_indices.keys())\n",
    "\n",
    "# âœ… Define CNN Model\n",
    "model = Sequential([\n",
    "    Conv2D(32, (3, 3), activation='relu', input_shape=(IMAGE_SIZE[0], IMAGE_SIZE[1], 3)),\n",
    "    MaxPooling2D(2, 2),\n",
    "\n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    MaxPooling2D(2, 2),\n",
    "\n",
    "    Conv2D(128, (3, 3), activation='relu'),\n",
    "    MaxPooling2D(2, 2),\n",
    "\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.5),  # Prevent Overfitting\n",
    "    Dense(len(CLASS_NAMES), activation='softmax')  # Output layer for multiple classes\n",
    "])\n",
    "\n",
    "# âœ… Compile Model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# âœ… Train Model\n",
    "EPOCHS = 15\n",
    "model.fit(train_generator, validation_data=val_generator, epochs=EPOCHS)\n",
    "\n",
    "# âœ… Save Model\n",
    "model_path = r\"C:\\Users\\bhish\\OneDrive\\Desktop\\Agriculture_Project\\Backend\\WeatherAnalysis\\my_model_ALL.keras\"\n",
    "model.save(model_path)\n",
    "print(f\"âœ… Model saved at {model_path}\")\n",
    "\n",
    "# -----------------------------------------------\n",
    "# ğŸš€ **Test Model on 20 Random Images from Test Set**\n",
    "# -----------------------------------------------\n",
    "# âœ… Get 20 Random Image Paths from Test Directory\n",
    "test_images = []\n",
    "for label in CLASS_NAMES:\n",
    "    label_folder = os.path.join(test_dir, label)\n",
    "    images = [os.path.join(label_folder, img) for img in os.listdir(label_folder) if img.endswith(('.jpg', '.png', '.jpeg'))]\n",
    "    test_images.extend(images)\n",
    "\n",
    "random.shuffle(test_images)  # Shuffle to pick random images\n",
    "test_images = test_images[:20]  # Select 20 random test images\n",
    "\n",
    "correct_predictions = 0\n",
    "total_predictions = len(test_images)\n",
    "\n",
    "# âœ… Function to Preprocess Image for Model\n",
    "def preprocess_image(image_path):\n",
    "    img = Image.open(image_path).convert(\"RGB\")\n",
    "    img = img.resize(IMAGE_SIZE)\n",
    "    img_array = np.array(img) / 255.0  # Normalize pixel values\n",
    "    img_array = np.expand_dims(img_array, axis=0)  # Add batch dimension\n",
    "    return img_array\n",
    "\n",
    "# âœ… Perform Predictions\n",
    "for img_path in test_images:\n",
    "    img_array = preprocess_image(img_path)\n",
    "    predictions = model.predict(img_array)\n",
    "\n",
    "    predicted_class = CLASS_NAMES[np.argmax(predictions[0])]\n",
    "    confidence = round(100 * np.max(predictions[0]), 2)\n",
    "\n",
    "    actual_class = img_path.split(os.path.sep)[-2]  # Get actual class from folder name\n",
    "\n",
    "    if predicted_class == actual_class:\n",
    "        correct_predictions += 1\n",
    "\n",
    "    print(f\"ğŸ“¸ Image: {os.path.basename(img_path)}\")\n",
    "    print(f\"âœ… Actual Class: {actual_class}\")\n",
    "    print(f\"ğŸ” Predicted Class: {predicted_class} (Confidence: {confidence}%)\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "# âœ… Calculate Accuracy\n",
    "accuracy = (correct_predictions / total_predictions) * 100\n",
    "print(f\"ğŸ¯ Model Accuracy on 20 Test Images: {accuracy:.2f}% ({correct_predictions}/{total_predictions} Correct Predictions)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
