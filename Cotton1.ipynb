{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8640 images belonging to 6 classes.\n",
      "Found 721 images belonging to 6 classes.\n",
      "Found 721 images belonging to 6 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bhish\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
      "C:\\Users\\bhish\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "\u001b[1m270/270\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m367s\u001b[0m 1s/step - accuracy: 0.2563 - loss: 2.0562 - val_accuracy: 0.5506 - val_loss: 1.2027\n",
      "Epoch 2/15\n",
      "\u001b[1m270/270\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m329s\u001b[0m 1s/step - accuracy: 0.4672 - loss: 1.3791 - val_accuracy: 0.5437 - val_loss: 1.1261\n",
      "Epoch 3/15\n",
      "\u001b[1m270/270\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m322s\u001b[0m 1s/step - accuracy: 0.5307 - loss: 1.2445 - val_accuracy: 0.5950 - val_loss: 1.0411\n",
      "Epoch 4/15\n",
      "\u001b[1m270/270\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m321s\u001b[0m 1s/step - accuracy: 0.5552 - loss: 1.1599 - val_accuracy: 0.6533 - val_loss: 0.8797\n",
      "Epoch 5/15\n",
      "\u001b[1m270/270\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m316s\u001b[0m 1s/step - accuracy: 0.5911 - loss: 1.1026 - val_accuracy: 0.7198 - val_loss: 0.8052\n",
      "Epoch 6/15\n",
      "\u001b[1m270/270\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m314s\u001b[0m 1s/step - accuracy: 0.6340 - loss: 1.0091 - val_accuracy: 0.6963 - val_loss: 0.8221\n",
      "Epoch 7/15\n",
      "\u001b[1m270/270\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m318s\u001b[0m 1s/step - accuracy: 0.6417 - loss: 0.9792 - val_accuracy: 0.7517 - val_loss: 0.6743\n",
      "Epoch 8/15\n",
      "\u001b[1m270/270\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m318s\u001b[0m 1s/step - accuracy: 0.6833 - loss: 0.8724 - val_accuracy: 0.7614 - val_loss: 0.6045\n",
      "Epoch 9/15\n",
      "\u001b[1m270/270\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m318s\u001b[0m 1s/step - accuracy: 0.6968 - loss: 0.8353 - val_accuracy: 0.7739 - val_loss: 0.6288\n",
      "Epoch 10/15\n",
      "\u001b[1m270/270\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m324s\u001b[0m 1s/step - accuracy: 0.7092 - loss: 0.8083 - val_accuracy: 0.7989 - val_loss: 0.5658\n",
      "Epoch 11/15\n",
      "\u001b[1m270/270\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m323s\u001b[0m 1s/step - accuracy: 0.7419 - loss: 0.7214 - val_accuracy: 0.8086 - val_loss: 0.5514\n",
      "Epoch 12/15\n",
      "\u001b[1m270/270\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m316s\u001b[0m 1s/step - accuracy: 0.7427 - loss: 0.7125 - val_accuracy: 0.8114 - val_loss: 0.4920\n",
      "Epoch 13/15\n",
      "\u001b[1m270/270\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m315s\u001b[0m 1s/step - accuracy: 0.7516 - loss: 0.7128 - val_accuracy: 0.8294 - val_loss: 0.4707\n",
      "Epoch 14/15\n",
      "\u001b[1m270/270\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m324s\u001b[0m 1s/step - accuracy: 0.7665 - loss: 0.6624 - val_accuracy: 0.8474 - val_loss: 0.4097\n",
      "Epoch 15/15\n",
      "\u001b[1m270/270\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m348s\u001b[0m 1s/step - accuracy: 0.7695 - loss: 0.6461 - val_accuracy: 0.8086 - val_loss: 0.5068\n",
      "âœ… Model saved at C:\\Users\\bhish\\OneDrive\\Desktop\\Agriculture_Project\\Backend\\WeatherAnalysis\\my_model11.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 170ms/step\n",
      "ğŸ“¸ Image: pil_sharpness_zoom_22.jpg\n",
      "âœ… Actual Class: Healthy\n",
      "ğŸ” Predicted Class: Healthy (Confidence: 99.80999755859375%)\n",
      "--------------------------------------------------\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "ğŸ“¸ Image: translation_zoom_6.jpg\n",
      "âœ… Actual Class: Aphids\n",
      "ğŸ” Predicted Class: Aphids (Confidence: 72.08000183105469%)\n",
      "--------------------------------------------------\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step\n",
      "ğŸ“¸ Image: translation_zoom_35.jpg\n",
      "âœ… Actual Class: Powdery_Mildew\n",
      "ğŸ” Predicted Class: Powdery_Mildew (Confidence: 99.94999694824219%)\n",
      "--------------------------------------------------\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step\n",
      "ğŸ“¸ Image: flip_horizontal_25.jpg\n",
      "âœ… Actual Class: Powdery_Mildew\n",
      "ğŸ” Predicted Class: Powdery_Mildew (Confidence: 94.80999755859375%)\n",
      "--------------------------------------------------\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step\n",
      "ğŸ“¸ Image: flip_horizontal_17.jpg\n",
      "âœ… Actual Class: Aphids\n",
      "ğŸ” Predicted Class: Aphids (Confidence: 97.7699966430664%)\n",
      "--------------------------------------------------\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step\n",
      "ğŸ“¸ Image: translation_zoom_9.jpg\n",
      "âœ… Actual Class: Bacterial_Blight\n",
      "ğŸ” Predicted Class: Bacterial_Blight (Confidence: 46.7400016784668%)\n",
      "--------------------------------------------------\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step\n",
      "ğŸ“¸ Image: contrast_25.jpg\n",
      "âœ… Actual Class: Powdery_Mildew\n",
      "ğŸ” Predicted Class: Powdery_Mildew (Confidence: 86.83999633789062%)\n",
      "--------------------------------------------------\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n",
      "ğŸ“¸ Image: translation_zoom_26.jpg\n",
      "âœ… Actual Class: Target_spot\n",
      "ğŸ” Predicted Class: Target_spot (Confidence: 46.709999084472656%)\n",
      "--------------------------------------------------\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "ğŸ“¸ Image: zoom_36.jpg\n",
      "âœ… Actual Class: Army_worm\n",
      "ğŸ” Predicted Class: Army_worm (Confidence: 98.66000366210938%)\n",
      "--------------------------------------------------\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "ğŸ“¸ Image: pil_constract_1.jpg\n",
      "âœ… Actual Class: Powdery_Mildew\n",
      "ğŸ” Predicted Class: Powdery_Mildew (Confidence: 99.97000122070312%)\n",
      "--------------------------------------------------\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "ğŸ“¸ Image: pil_sharpness_23.jpg\n",
      "âœ… Actual Class: Powdery_Mildew\n",
      "ğŸ” Predicted Class: Powdery_Mildew (Confidence: 63.09000015258789%)\n",
      "--------------------------------------------------\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "ğŸ“¸ Image: zoom_14.jpg\n",
      "âœ… Actual Class: Aphids\n",
      "ğŸ” Predicted Class: Aphids (Confidence: 85.88999938964844%)\n",
      "--------------------------------------------------\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "ğŸ“¸ Image: constract_high_zoom_15.jpg\n",
      "âœ… Actual Class: Healthy\n",
      "ğŸ” Predicted Class: Bacterial_Blight (Confidence: 51.709999084472656%)\n",
      "--------------------------------------------------\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "ğŸ“¸ Image: constract_high_zoom_19.jpg\n",
      "âœ… Actual Class: Target_spot\n",
      "ğŸ” Predicted Class: Target_spot (Confidence: 66.25%)\n",
      "--------------------------------------------------\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "ğŸ“¸ Image: crop_12.jpg\n",
      "âœ… Actual Class: Army_worm\n",
      "ğŸ” Predicted Class: Army_worm (Confidence: 96.98999786376953%)\n",
      "--------------------------------------------------\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step\n",
      "ğŸ“¸ Image: rotation_30.jpg\n",
      "âœ… Actual Class: Bacterial_Blight\n",
      "ğŸ” Predicted Class: Bacterial_Blight (Confidence: 96.58999633789062%)\n",
      "--------------------------------------------------\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "ğŸ“¸ Image: crop_5.jpg\n",
      "âœ… Actual Class: Target_spot\n",
      "ğŸ” Predicted Class: Target_spot (Confidence: 88.79000091552734%)\n",
      "--------------------------------------------------\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "ğŸ“¸ Image: crop_9.png\n",
      "âœ… Actual Class: Target_spot\n",
      "ğŸ” Predicted Class: Target_spot (Confidence: 70.44999694824219%)\n",
      "--------------------------------------------------\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "ğŸ“¸ Image: flip_horizontal_14.jpg\n",
      "âœ… Actual Class: Target_spot\n",
      "ğŸ” Predicted Class: Bacterial_Blight (Confidence: 44.810001373291016%)\n",
      "--------------------------------------------------\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "ğŸ“¸ Image: constract_high_39.jpg\n",
      "âœ… Actual Class: Target_spot\n",
      "ğŸ” Predicted Class: Target_spot (Confidence: 91.19999694824219%)\n",
      "--------------------------------------------------\n",
      "ğŸ¯ Model Accuracy on 20 Test Images: 90.00% (18/20 Correct Predictions)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import random\n",
    "\n",
    "# âœ… Dataset Paths (Modify These Paths as Needed)\n",
    "train_dir = r\"C:\\Users\\bhish\\OneDrive\\Desktop\\ML\\Cotton\\cotton_dataset\\train\"\n",
    "val_dir = r\"C:\\Users\\bhish\\OneDrive\\Desktop\\ML\\Cotton\\cotton_dataset\\test\"\n",
    "test_dir = r\"C:\\Users\\bhish\\OneDrive\\Desktop\\ML\\Cotton\\cotton_dataset\\test\"\n",
    "\n",
    "# âœ… Image Parameters\n",
    "IMAGE_SIZE = (256, 256)\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# âœ… Data Augmentation & Normalization\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,  # Normalize pixel values\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode=\"nearest\"\n",
    ")\n",
    "\n",
    "val_datagen = ImageDataGenerator(rescale=1./255)  # Only Normalization\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)  # Normalize test images\n",
    "\n",
    "# âœ… Load Dataset\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_dir, target_size=IMAGE_SIZE, batch_size=BATCH_SIZE, class_mode='categorical')\n",
    "\n",
    "val_generator = val_datagen.flow_from_directory(\n",
    "    val_dir, target_size=IMAGE_SIZE, batch_size=BATCH_SIZE, class_mode='categorical')\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    test_dir, target_size=IMAGE_SIZE, batch_size=1, class_mode='categorical', shuffle=False)\n",
    "\n",
    "# âœ… Get Class Labels\n",
    "CLASS_NAMES = list(train_generator.class_indices.keys())\n",
    "\n",
    "# âœ… Define CNN Model\n",
    "model = Sequential([\n",
    "    Conv2D(32, (3, 3), activation='relu', input_shape=(IMAGE_SIZE[0], IMAGE_SIZE[1], 3)),\n",
    "    MaxPooling2D(2, 2),\n",
    "\n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    MaxPooling2D(2, 2),\n",
    "\n",
    "    Conv2D(128, (3, 3), activation='relu'),\n",
    "    MaxPooling2D(2, 2),\n",
    "\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.5),  # Prevent Overfitting\n",
    "    Dense(len(CLASS_NAMES), activation='softmax')  # Output layer for multiple classes\n",
    "])\n",
    "\n",
    "# âœ… Compile Model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# âœ… Train Model\n",
    "EPOCHS = 15\n",
    "model.fit(train_generator, validation_data=val_generator, epochs=EPOCHS)\n",
    "\n",
    "# âœ… Save Model\n",
    "model_path = r\"C:\\Users\\bhish\\OneDrive\\Desktop\\Agriculture_Project\\Backend\\WeatherAnalysis\\my_model11.keras\"\n",
    "model.save(model_path)\n",
    "print(f\"âœ… Model saved at {model_path}\")\n",
    "\n",
    "# -----------------------------------------------\n",
    "# ğŸš€ **Test Model on 20 Random Images from Test Set**\n",
    "# -----------------------------------------------\n",
    "# âœ… Get 20 Random Image Paths from Test Directory\n",
    "test_images = []\n",
    "for label in CLASS_NAMES:\n",
    "    label_folder = os.path.join(test_dir, label)\n",
    "    images = [os.path.join(label_folder, img) for img in os.listdir(label_folder) if img.endswith(('.jpg', '.png', '.jpeg'))]\n",
    "    test_images.extend(images)\n",
    "\n",
    "random.shuffle(test_images)  # Shuffle to pick random images\n",
    "test_images = test_images[:20]  # Select 20 random test images\n",
    "\n",
    "correct_predictions = 0\n",
    "total_predictions = len(test_images)\n",
    "\n",
    "# âœ… Function to Preprocess Image for Model\n",
    "def preprocess_image(image_path):\n",
    "    img = Image.open(image_path).convert(\"RGB\")\n",
    "    img = img.resize(IMAGE_SIZE)\n",
    "    img_array = np.array(img) / 255.0  # Normalize pixel values\n",
    "    img_array = np.expand_dims(img_array, axis=0)  # Add batch dimension\n",
    "    return img_array\n",
    "\n",
    "# âœ… Perform Predictions\n",
    "for img_path in test_images:\n",
    "    img_array = preprocess_image(img_path)\n",
    "    predictions = model.predict(img_array)\n",
    "\n",
    "    predicted_class = CLASS_NAMES[np.argmax(predictions[0])]\n",
    "    confidence = round(100 * np.max(predictions[0]), 2)\n",
    "\n",
    "    actual_class = img_path.split(os.path.sep)[-2]  # Get actual class from folder name\n",
    "\n",
    "    if predicted_class == actual_class:\n",
    "        correct_predictions += 1\n",
    "\n",
    "    print(f\"ğŸ“¸ Image: {os.path.basename(img_path)}\")\n",
    "    print(f\"âœ… Actual Class: {actual_class}\")\n",
    "    print(f\"ğŸ” Predicted Class: {predicted_class} (Confidence: {confidence}%)\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "# âœ… Calculate Accuracy\n",
    "accuracy = (correct_predictions / total_predictions) * 100\n",
    "print(f\"ğŸ¯ Model Accuracy on 20 Test Images: {accuracy:.2f}% ({correct_predictions}/{total_predictions} Correct Predictions)\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
